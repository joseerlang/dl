{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:22:46.196736Z",
     "start_time": "2020-06-17T17:22:46.165736Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/dl/blob/master/sentiment_analysis_bidirectional/sentiment_analysis_bidirectional.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:45.628297Z",
     "start_time": "2020-06-17T17:22:46.214737Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "TEXT = torchtext.data.Field(tokenize = 'spacy')\n",
    "LABEL = torchtext.data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:45.692295Z",
     "start_time": "2020-06-17T17:23:45.629298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:45.756297Z",
     "start_time": "2020-06-17T17:23:45.693297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy', '.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', ',', 'such', 'as', '\"', 'Teachers', '\"', '.', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', 'High', \"'s\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"', 'Teachers', '\"', '.', 'The', 'scramble', 'to', 'survive', 'financially', ',', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', \"'\", 'pomp', ',', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', ',', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students', '.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', ',', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High', '.', 'A', 'classic', 'line', ':', 'INSPECTOR', ':', 'I', \"'m\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'STUDENT', ':', 'Welcome', 'to', 'Bromwell', 'High', '.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched', '.', 'What', 'a', 'pity', 'that', 'it', 'is', \"n't\", '!'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:45.836297Z",
     "start_time": "2020-06-17T17:23:45.757297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17500, 7500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, valid_data = train_data.split()\n",
    "len(train_data), len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:46.836359Z",
     "start_time": "2020-06-17T17:23:45.837296Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:46.884362Z",
     "start_time": "2020-06-17T17:23:46.837361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab), len(LABEL.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two extra tokens: *unk* and *pad*. The *pad* token is used to ensure that all the sentences have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:46.963359Z",
     "start_time": "2020-06-17T17:23:46.885362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 203838),\n",
       " (',', 193608),\n",
       " ('.', 165578),\n",
       " ('and', 110025),\n",
       " ('a', 109623),\n",
       " ('of', 101283),\n",
       " ('to', 93879),\n",
       " ('is', 76833),\n",
       " ('in', 61708),\n",
       " ('I', 54122)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:47.026361Z",
     "start_time": "2020-06-17T17:23:46.965362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:47.090361Z",
     "start_time": "2020-06-17T17:23:47.028362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {'pos': 0, 'neg': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:47.169361Z",
     "start_time": "2020-06-17T17:23:47.091362Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataloader = {\n",
    "    'train': torchtext.data.BucketIterator(train_data, batch_size=64, sort_within_batch=True, device=device),\n",
    "    'val': torchtext.data.BucketIterator(valid_data, batch_size=64, device=device),\n",
    "    'test': torchtext.data.BucketIterator(test_data, batch_size=64, device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:47.230359Z",
     "start_time": "2020-06-17T17:23:47.170360Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metric():\n",
    "  def __init__(self):\n",
    "    self.name = \"acc\"\n",
    "  \n",
    "  def call(self, outputs, targets):\n",
    "    rounded_preds = torch.round(torch.sigmoid(outputs))\n",
    "    correct = (rounded_preds == targets).float() \n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:47.294360Z",
     "start_time": "2020-06-17T17:23:47.231359Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=128, hidden_dim=128, output_dim=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = torch.nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]        \n",
    "        embedded = self.embedding(text)        \n",
    "        #embedded = [sent len, batch size, emb dim]        \n",
    "        output, hidden = self.rnn(embedded)        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        y = self.fc(output[-1,:,:].squeeze(0)).squeeze(1)     \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell the network NOT to learn the pad token, since it is irrelevant. This is called *masking*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:47.358359Z",
     "start_time": "2020-06-17T17:23:47.295359Z"
    }
   },
   "outputs": [],
   "source": [
    "class MaskedRNN(RNN):\n",
    "    def __init__(self, input_dim, embedding_dim=128, hidden_dim=128, output_dim=1, dropout=0.2, pad_idx=0):\n",
    "        super().__init__(input_dim, embedding_dim, hidden_dim, output_dim, dropout)\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidireccional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step, a regular recurrent layer only looks at past and present inputs before generating an output. This makes sense for time series forecasting, but for some NLP tasks it is preferable to look ahead at the next words before encoding a given word. We can achieve this using bidirectional recurrent layers.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:47.596360Z",
     "start_time": "2020-06-17T17:23:47.359360Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "from src import WordModel\n",
    "\n",
    "class MaskedBidirectionalRNN(MaskedRNN):\n",
    "    def __init__(self, input_dim, embedding_dim=128, hidden_dim=128, output_dim=1, dropout=0.2, pad_idx=0, bidirectional=True):\n",
    "        super().__init__(input_dim, embedding_dim, hidden_dim, output_dim, dropout)\n",
    "        self.rnn = torch.nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2, dropout=dropout, bidirectional=bidirectional)\n",
    "        if bidirectional:\n",
    "          self.fc = torch.nn.Linear(2*hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:23:47.676359Z",
     "start_time": "2020-06-17T17:23:47.597360Z"
    }
   },
   "outputs": [],
   "source": [
    "net = MaskedBidirectionalRNN(len(TEXT.vocab), embedding_dim=100, pad_idx=TEXT.vocab.stoi[TEXT.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T17:22:46.377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      60.00% [3/5 00:50<00:33]\n",
       "    </div>\n",
       "    \n",
       "Epoch 1/5 loss 0.68767 acc 0.53317 val_loss 0.71789 val_acc 0.49559<p>Epoch 2/5 loss 0.58774 acc 0.69445 val_loss 0.78800 val_acc 0.56497<p>Epoch 3/5 loss 0.45562 acc 0.79721 val_loss 0.79778 val_acc 0.63445<p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='220' class='' max='274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      80.29% [220/274 00:09<00:02 train_loss 0.37390 train_acc 0.84595]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = WordModel(net)\n",
    "\n",
    "model.compile(optimizer = torch.optim.Adam(model.net.parameters()),\n",
    "              loss = torch.nn.BCEWithLogitsLoss(),\n",
    "              metrics=[Metric()])\n",
    "\n",
    "hist = model.fit(dataloader['train'], dataloader['val'], epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T17:22:46.378Z"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(dataloader['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our model to predict if a movie review is good or bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T17:22:46.406Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T17:22:46.408Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\"this film is terrible\", \"this film is great\", \"this film is good\", \"a waste of time\"]\n",
    "tokenized = [[tok.text for tok in nlp.tokenizer(sentence)] for sentence in sentences]\n",
    "indexed = [[TEXT.vocab.stoi[_t] for _t in t] for t in tokenized]\n",
    "tensor = torch.tensor(indexed).to(device).permute(1,0)\n",
    "model.net.eval()\n",
    "prediction = torch.sigmoid(model.net(tensor))\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
