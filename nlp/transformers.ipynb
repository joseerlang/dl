{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T16:25:20.217775Z",
     "start_time": "2020-06-17T16:25:20.188777Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/dl/blob/master/sentiment_analysis_bidirectional/sentiment_analysis_bidirectional.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T16:52:45.675Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T16:52:45.817Z"
    }
   },
   "source": [
    "Attention mechanisms allows a model to focus only on the appropiate words at each time step. They revolutionized NLP allowing significant improvements in the state of the art. This technique is applied in the [*Transformer*](https://arxiv.org/abs/1706.03762) architecture, where recurrent layers are replaced by attention layers achieving better performance. Transformers are growing in popularity, and new versions appear constantly. One recent interesting model is [BERT](https://arxiv.org/abs/1810.04805). A popular library to work with this models is [transformers](https://github.com/huggingface/transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:16:15.569457Z",
     "start_time": "2020-06-17T17:16:06.783618Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (1.18.4)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp37-cp37m-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (4.42.1)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Collecting packaging\n",
      "  Downloading packaging-20.4-py2.py3-none-any.whl (37 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.6.8-cp37-cp37m-win_amd64.whl (268 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from requests->transformers) (2020.4.5.2)\n",
      "Requirement already satisfied: six in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893262 sha256=51b03cf3e71313416cb23c35388a2c41a2f8c0bd369308ef3bd2e34ef7d6fa34\n",
      "  Stored in directory: c:\\users\\sensio\\appdata\\local\\pip\\cache\\wheels\\69\\09\\d1\\bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: filelock, sentencepiece, tokenizers, regex, click, sacremoses, packaging, transformers\n",
      "Successfully installed click-7.1.2 filelock-3.0.12 packaging-20.4 regex-2020.6.8 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:16:20.788367Z",
     "start_time": "2020-06-17T17:16:15.571456Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:16:20.804367Z",
     "start_time": "2020-06-17T17:16:20.789368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:16:21.970390Z",
     "start_time": "2020-06-17T17:16:21.960389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:16:27.157624Z",
     "start_time": "2020-06-17T17:16:27.150624Z"
    }
   },
   "source": [
    "We need our custom tokenizer, that also cuts sentences to the maximum number of tokens required by BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:16:34.573371Z",
     "start_time": "2020-06-17T17:16:34.557371Z"
    }
   },
   "outputs": [],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:16:49.850785Z",
     "start_time": "2020-06-17T17:16:49.817782Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "TEXT = torchtext.data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = tokenizer.cls_token_id,\n",
    "                  eos_token = tokenizer.sep_token_id,\n",
    "                  pad_token = tokenizer.pad_token_id,\n",
    "                  unk_token = tokenizer.unk_token_id)\n",
    "\n",
    "LABEL = torchtext.data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataloader = {\n",
    "    'train': torchtext.data.BucketIterator(train_data, batch_size=64, sort_within_batch=True, device=device),\n",
    "    'val': torchtext.data.BucketIterator(valid_data, batch_size=64, device=device),\n",
    "    'test': torchtext.data.BucketIterator(test_data, batch_size=64, device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our BERT-based model for sentiment analysis will use BERT as the embedding layer. Then, the outputs will be passed to a bidirectional GRU as we did in the previous examples. Also, we will NOT train the weights from BERT. This is called freezing the network, and will speed up calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim=256, output_dim=1, n_layers=2, bidirectional=True, dropout=0.2):\n",
    "        super().__init__()        \n",
    "        self.bert = bert        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):                       \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "        _, hidden = self.rnn(embedded)        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])        \n",
    "        output = self.out(hidden)        \n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BERTGRUSentiment(bert)\n",
    "\n",
    "# freeze BERT\n",
    "for name, param in net.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import WordModel\n",
    "\n",
    "model = WordModel(net)\n",
    "\n",
    "model.compile(optimizer = torch.optim.Adam(model.net.parameters()),\n",
    "              loss = torch.nn.BCEWithLogitsLoss(),\n",
    "              metrics=[Metric()])\n",
    "\n",
    "hist = model.fit(train_data, validation_data=valid_data, epochs=1)\n",
    "\n",
    "hist = model.fit(dataloader['train'], dataloader['val'], epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"the film was good\"\n",
    "tokenized = [tok[:max_input_length-2] for tok in tokenizer.tokenize(sentence)]\n",
    "indexed = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokenized) + [tokenizer.sep_token_id]\n",
    "tensor = torch.tensor([indexed]).to(device)\n",
    "model.net.eval()\n",
    "prediction = torch.sigmoid(model.net(tensor))\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
