{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:25:24.465902Z",
     "start_time": "2020-06-17T17:25:24.435901Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/dl/blob/master/nlp/transformers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T16:52:45.817Z"
    }
   },
   "source": [
    "Attention mechanisms allows a model to focus only on the appropiate words at each time step. They revolutionized NLP allowing significant improvements in the state of the art. This technique is applied in the [*Transformer*](https://arxiv.org/abs/1706.03762) architecture, where recurrent layers are replaced by attention layers achieving better performance. Transformers are growing in popularity, and new versions appear constantly. One recent interesting model is [BERT](https://arxiv.org/abs/1810.04805). A popular library to work with this models is [transformers](https://github.com/huggingface/transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:25:26.773084Z",
     "start_time": "2020-06-17T17:25:25.544915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sensio\\miniconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: numpy in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (1.18.4)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: click in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (0.15.1)\n",
      "Requirement already satisfied: six in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from requests->transformers) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\sensio\\miniconda3\\lib\\site-packages (from requests->transformers) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:25:30.153302Z",
     "start_time": "2020-06-17T17:25:26.775082Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:25:30.277824Z",
     "start_time": "2020-06-17T17:25:30.155302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:25:30.401343Z",
     "start_time": "2020-06-17T17:25:30.279338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088, 2129, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:16:27.157624Z",
     "start_time": "2020-06-17T17:16:27.150624Z"
    }
   },
   "source": [
    "We need our custom tokenizer, that also cuts sentences to the maximum number of tokens required by BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:25:30.529337Z",
     "start_time": "2020-06-17T17:25:30.402343Z"
    }
   },
   "outputs": [],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:25:30.685337Z",
     "start_time": "2020-06-17T17:25:30.530337Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "TEXT = torchtext.data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = tokenizer.cls_token_id,\n",
    "                  eos_token = tokenizer.sep_token_id,\n",
    "                  pad_token = tokenizer.pad_token_id,\n",
    "                  unk_token = tokenizer.unk_token_id)\n",
    "\n",
    "LABEL = torchtext.data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:28:43.438571Z",
     "start_time": "2020-06-17T17:25:30.687341Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train_data, valid_data = train_data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:28:43.582572Z",
     "start_time": "2020-06-17T17:28:43.439572Z"
    }
   },
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:28:43.723572Z",
     "start_time": "2020-06-17T17:28:43.583573Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataloader = {\n",
    "    'train': torchtext.data.BucketIterator(train_data, batch_size=64, sort_within_batch=True, device=device),\n",
    "    'val': torchtext.data.BucketIterator(valid_data, batch_size=64, device=device),\n",
    "    'test': torchtext.data.BucketIterator(test_data, batch_size=64, device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:29:32.757999Z",
     "start_time": "2020-06-17T17:28:43.724572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabdbeea0d0a42e78b73d6f5c9dd3d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cafadeed5841e3818684469c58456e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our BERT-based model for sentiment analysis will use BERT as the embedding layer. Then, the outputs will be passed to a bidirectional GRU as we did in the previous examples. Also, we will NOT train the weights from BERT. This is called freezing the network, and will speed up calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:29:32.885383Z",
     "start_time": "2020-06-17T17:29:32.759001Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim=256, output_dim=1, n_layers=2, bidirectional=True, dropout=0.2):\n",
    "        super().__init__()        \n",
    "        self.bert = bert        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):                       \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "        _, hidden = self.rnn(embedded)        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])        \n",
    "        output = self.out(hidden)        \n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:37:39.109140Z",
     "start_time": "2020-06-17T17:37:38.975635Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metric():\n",
    "  def __init__(self):\n",
    "    self.name = \"acc\"\n",
    "  \n",
    "  def call(self, outputs, targets):\n",
    "    rounded_preds = torch.round(torch.sigmoid(outputs))\n",
    "    correct = (rounded_preds == targets).float() \n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T17:37:43.530102Z",
     "start_time": "2020-06-17T17:37:43.384102Z"
    }
   },
   "outputs": [],
   "source": [
    "net = BERTGRUSentiment(bert)\n",
    "\n",
    "# freeze BERT\n",
    "for name, param in net.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T18:20:10.824699Z",
     "start_time": "2020-06-17T17:37:49.452467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Epoch 1/5 loss 0.37754 acc 0.82602 val_loss 0.30273 val_acc 0.87125<p>Epoch 2/5 loss 0.24373 acc 0.90324 val_loss 0.25205 val_acc 0.89994<p>Epoch 3/5 loss 0.20090 acc 0.92009 val_loss 0.21851 val_acc 0.91737<p>Epoch 4/5 loss 0.17466 acc 0.93279 val_loss 0.22887 val_acc 0.91062<p>Epoch 5/5 loss 0.13960 acc 0.94722 val_loss 0.22662 val_acc 0.90956"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src import WordModel\n",
    "\n",
    "model = WordModel(net)\n",
    "\n",
    "model.compile(optimizer = torch.optim.Adam(model.net.parameters()),\n",
    "              loss = torch.nn.BCEWithLogitsLoss(),\n",
    "              metrics=[Metric()])\n",
    "\n",
    "hist = model.fit(dataloader['train'], dataloader['val'], epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T18:32:21.159961Z",
     "start_time": "2020-06-17T18:20:15.603624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "loss 0.21621 acc 0.91503"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.evaluate(dataloader['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T18:54:57.136495Z",
     "start_time": "2020-06-17T18:54:56.997484Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    tokenized = [tok[:max_input_length-2] for tok in tokenizer.tokenize(sentence)]\n",
    "    indexed = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokenized) + [tokenizer.sep_token_id]\n",
    "    tensor = torch.tensor([indexed]).to(device)\n",
    "    model.net.eval()\n",
    "    return torch.sigmoid(model.net(tensor)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T18:58:13.282258Z",
     "start_time": "2020-06-17T18:58:13.105263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.012061146087944508, 0.9844273924827576]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Best film ever !\", \"this movie is terrible\"]\n",
    "preds = [predict(s) for s in sentences]\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
