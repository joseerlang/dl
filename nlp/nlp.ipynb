{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:48.003554Z",
     "start_time": "2020-06-17T15:03:47.972555Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Languaje Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚡ NLP is a very active field at the moment, new huge architectures (Transformers) and training techniques (language modeling on big unsupervised datasets) are providing excellent results improving SOTA by large margin on almost  every task. See for example: https://openai.com/blog/openai-api/. Some applications include:\n",
    "\n",
    "- Language comprehension (virtual assistants such as Siri, Alexa, …) \n",
    "- Machine translation (Google Translate, …)\n",
    "- Text generation (language modeling, summarization, question answering)\n",
    "- Text classification(sentiment analysis, identify hate speech on social media, ...)\n",
    "- Text-to-speech (generate audio from text) and Speech-to-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate Shakespearean text using a character RNN (*CharRNN*, https://github.com/karpathy/char-rnn). A CharRNN is able to generate new text, one character at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:48.818556Z",
     "start_time": "2020-06-17T15:03:48.035555Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 1115394 / 1115394"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'shakespeare (3).txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download dataset\n",
    "\n",
    "import wget\n",
    "\n",
    "url = \"https://mymldatasets.s3.eu-de.cloud-object-storage.appdomain.cloud/shakespeare.txt\"\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:48.865555Z",
     "start_time": "2020-06-17T15:03:48.819555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou',\n",
       " 1115394)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load file\n",
    "\n",
    "f = open(\"shakespeare.txt\", \"r\")\n",
    "text = f.read()\n",
    "text[:100], len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to encode every character in the text as an integer. This process is called **tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:48.896555Z",
     "start_time": "2020-06-17T15:03:48.867555Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Tokenizer(): \n",
    "  def __init__(self):\n",
    "    self.all_characters = string.printable\n",
    "    self.n_characters = len(self.all_characters)\n",
    "  def text_to_seq(self, string):\n",
    "    seq = []\n",
    "    for c in range(len(string)):\n",
    "        seq.append(self.all_characters.index(string[c]))\n",
    "    return seq\n",
    "  def seq_to_text(self, seq):\n",
    "    text = ''\n",
    "    for c in range(len(seq)):\n",
    "        text += self.all_characters[seq[c]]\n",
    "    return text\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:48.928554Z",
     "start_time": "2020-06-17T15:03:48.898556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:48.958555Z",
     "start_time": "2020-06-17T15:03:48.929556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:48.989557Z",
     "start_time": "2020-06-17T15:03:48.959555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 12, 39, 40, 41]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.text_to_seq('abcDEF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:49.020557Z",
     "start_time": "2020-06-17T15:03:48.990557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.seq_to_text([10, 11, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:49.298556Z",
     "start_time": "2020-06-17T15:03:49.022557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41, 18, 27, 28, 29, 94, 38, 18, 29, 18]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoded = tokenizer.text_to_seq(text)\n",
    "text_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:49.330557Z",
     "start_time": "2020-06-17T15:03:49.300557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citi'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's split the dataset into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:49.378555Z",
     "start_time": "2020-06-17T15:03:49.332554Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_size = len(text_encoded) * 90 // 100 # keep 90% for training\n",
    "train = text_encoded[:train_size]\n",
    "test = text_encoded[-train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To train an RNN we need to chop the long sequence of text into multiple windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:49.674554Z",
     "start_time": "2020-06-17T15:03:49.380556Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def windows(text, window_size = 100):\n",
    "    start_index = 0\n",
    "    end_index = len(text) - window_size\n",
    "    text_windows = []\n",
    "    while start_index < end_index:\n",
    "      text_windows.append(text[start_index:start_index+window_size+1])\n",
    "      start_index += 1\n",
    "    return text_windows\n",
    "\n",
    "text_windows = windows(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:49.705557Z",
     "start_time": "2020-06-17T15:03:49.675556Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ',\n",
       " 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou a',\n",
       " 'rst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ar']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_windows[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:50.286555Z",
     "start_time": "2020-06-17T15:03:49.706556Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CharRNNDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, text_encoded_windows, train=True):\n",
    "    self.text = text_encoded_windows\n",
    "    self.train = train\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "\n",
    "  def __getitem__(self, ix):\n",
    "    if self.train:\n",
    "      return torch.tensor(self.text[ix][:-1]), torch.tensor(self.text[ix][-1])\n",
    "    return torch.tensor(self.text[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:53.130617Z",
     "start_time": "2020-06-17T15:03:50.287556Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "text_encoded_windows = windows(train)\n",
    "\n",
    "# get a small subset\n",
    "dataset = CharRNNDataset(text_encoded_windows)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:53.176618Z",
     "start_time": "2020-06-17T15:03:53.131617Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all r'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, output = dataset[10]\n",
    "tokenizer.seq_to_text(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:53.221617Z",
     "start_time": "2020-06-17T15:03:53.177618Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.seq_to_text([output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since each character is a category, we need to transform the inputs to either one-hot vectors or as embeddings. Here, we will use embeddings.\n",
    "\n",
    "An embedding is a trainable dense vector that represents a category. The number of dimensions of the embedding is a hyperparameter that can be tweaked. Since embeddings are trainable, they will improve during training grouping together similar categories. The better the representation, the easier it will be for the neural network to make accurate predictions. This is also called *representation learning*. When applied to NLP tasks, we generally talk about *word embeddings*, which can generate results such as \n",
    "\n",
    "![](https://blog.enzymeadvisinggroup.com/hs-fs/hubfs/Word%20Embeddings%20en%20el%20Natural%20Language%20Processing.png?width=1505&name=Word%20Embeddings%20en%20el%20Natural%20Language%20Processing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T15:03:53.267617Z",
     "start_time": "2020-06-17T15:03:53.222618Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "  def __init__(self, n_out=100, dropout=0.2, input_size=100, embedding_size=100, hidden_size=128):\n",
    "    super().__init__()\n",
    "    self.encoder = torch.nn.Embedding(input_size, embedding_size)\n",
    "    self.rnn = torch.nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=2, dropout=dropout, batch_first=True)\n",
    "    self.fc = torch.nn.Linear(hidden_size, n_out)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x, h = self.rnn(x)         \n",
    "    y = self.fc(x[:,-1,:])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T15:03:48.189Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\sensio\\miniconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/5 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='11990' class='' max='15684' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      76.45% [11990/15684 02:54<00:53 train_loss 1.78878]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src import CharRNNModel\n",
    "\n",
    "model = CharRNNModel(CharRNN())\n",
    "\n",
    "model.compile(optimizer = torch.optim.Adam(model.net.parameters()),\n",
    "              loss = torch.nn.CrossEntropyLoss())\n",
    "\n",
    "hist = model.fit(dataloader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T15:03:48.205Z"
    }
   },
   "outputs": [],
   "source": [
    "text[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T15:03:48.206Z"
    }
   },
   "outputs": [],
   "source": [
    "X_new = \"With eyes wide open; standing, speaking, movin\"\n",
    "X_new_encoded = tokenizer.text_to_seq(X_new)\n",
    "y_pred = model.predict(X_new_encoded)\n",
    "y_pred = torch.argmax(y_pred, axis=1)[0].item()\n",
    "tokenizer.seq_to_text([y_pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start generating fake text. We could start with a seed and then recursively generate text, adding the new characters to the existing ones. This approach, however, results in repetitive text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T15:03:48.224Z"
    }
   },
   "outputs": [],
   "source": [
    "X_new = \"With eyes wide open; standing, speaking, movin\"\n",
    "\n",
    "for i in range(1000):\n",
    "  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])\n",
    "  y_pred = model.predict(X_new_encoded)\n",
    "  y_pred = torch.argmax(y_pred, axis=1)[0].item()\n",
    "  X_new += tokenizer.seq_to_text([y_pred])\n",
    "\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pick the next character randomly, with a probability equal to the estimated probability. This will generate more diverse and interesting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-17T15:03:48.239Z"
    }
   },
   "outputs": [],
   "source": [
    "X_new = \"With eyes wide open; standing, speaking, movin\"\n",
    "\n",
    "temp= 0.5\n",
    "for i in range(1000):\n",
    "  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])\n",
    "  y_pred = model.predict(X_new_encoded)\n",
    "  y_pred = y_pred.view(-1).div(temp).exp()\n",
    "  top_i = torch.multinomial(y_pred, 1)[0]\n",
    "  predicted_char = tokenizer.all_characters[top_i]\n",
    "  X_new += predicted_char\n",
    "\n",
    "print(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
